{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOiH+YMWIPW7v/ix04EgkuB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matsu641/APS360Project/blob/main/APS360_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbk-mEuod5O5",
        "outputId": "33961119-ca53-472b-881b-a8024e9afbdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpVWri4idkcM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 2. Data preprocessing\n",
        "data_dir = \"/content/drive/MyDrive/NIH_ChestXray_subset\"\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),      # Resize all images to 128x128\n",
        "    transforms.ToTensor(),              # Convert to tensor\n",
        "    transforms.Normalize((0.5,), (0.5,)) # Normalize to [-1, 1]\n",
        "])\n",
        "\n",
        "train_dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size   = len(train_dataset) - train_size\n",
        "train_set, val_set = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "val_loader   = DataLoader(val_set, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "bVXs4UgLdsUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Simple CNN architecture (baseline)\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=4):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "        )\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(32 * 32 * 32, 128), # After 2 poolings on 128x128\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n",
        "\n",
        "model = SimpleCNN(num_classes=4).to(device)"
      ],
      "metadata": {
        "id": "ML8nzRBfd_h7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "6CPyagW6eq8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Training loop\n",
        "num_epochs = 5  # keep small for baseline\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    train_acc = 100 * correct / total\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
        "          f\"Loss: {running_loss/len(train_loader):.4f}, \"\n",
        "          f\"Train Acc: {train_acc:.2f}%\")\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_correct, val_total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = outputs.max(1)\n",
        "            val_correct += predicted.eq(labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "    val_acc = 100 * val_correct / val_total\n",
        "    print(f\"Validation Acc: {val_acc:.2f}%\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LL6NXjfbetYN",
        "outputId": "04eeccca-6f5e-4bce-ff6d-e805c2dfed59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 0.3418, Train Acc: 92.23%\n",
            "Validation Acc: 95.49%\n",
            "\n",
            "Epoch [2/5], Loss: 0.2976, Train Acc: 92.67%\n",
            "Validation Acc: 95.49%\n",
            "\n",
            "Epoch [3/5], Loss: 0.2893, Train Acc: 92.69%\n",
            "Validation Acc: 95.49%\n",
            "\n",
            "Epoch [4/5], Loss: 0.2693, Train Acc: 92.77%\n",
            "Validation Acc: 95.49%\n",
            "\n",
            "Epoch [5/5], Loss: 0.2450, Train Acc: 92.77%\n",
            "Validation Acc: 95.49%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Save baseline model\n",
        "torch.save(model.state_dict(), \"baseline_cnn.pth\")\n",
        "print(\"Model saved as baseline_cnn.pth\")"
      ],
      "metadata": {
        "id": "brcDL55oext0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caece86d-0612-489f-c0ef-0b4dfd535fd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as baseline_cnn.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Evaluate the result properly\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Collect predictions and labels\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, preds = outputs.max(1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(all_labels, all_preds))\n",
        "\n",
        "# Precision, Recall, F1 per class\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=val_loader.dataset.dataset.classes))\n"
      ],
      "metadata": {
        "id": "KOJWaOHkoZCz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be5420a5-8aed-47f2-9f11-924e02e2382c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[   0    1   13    0]\n",
            " [   0    0   39    0]\n",
            " [   0    0 1229    0]\n",
            " [   0    0    5    0]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "Cardiomegaly       0.00      0.00      0.00        14\n",
            "    Effusion       0.00      0.00      0.00        39\n",
            "  No Finding       0.96      1.00      0.98      1229\n",
            "   Pneumonia       0.00      0.00      0.00         5\n",
            "\n",
            "    accuracy                           0.95      1287\n",
            "   macro avg       0.24      0.25      0.24      1287\n",
            "weighted avg       0.91      0.95      0.93      1287\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ]
}