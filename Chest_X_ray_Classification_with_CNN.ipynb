{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wD-nA_Ldyqh9",
        "outputId": "5bb5b0e8-7890-48eb-f95c-7194b8ea84f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7V7K1n60Wsa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoUmJ8xSEVD7"
      },
      "source": [
        "# **Base line code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcMikcvr_aGq"
      },
      "outputs": [],
      "source": [
        "# ======================\n",
        "# 1. Data preparation\n",
        "# ======================\n",
        "data_dir = \"/content/drive/MyDrive/chest_xray\"\n",
        "\n",
        "# Transform: resize, convert to tensor, normalize as ImageNet pretrained model expects\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load datasets (train, val, test folders already exist in Kaggle dataset)\n",
        "train_dataset = datasets.ImageFolder(root=f\"{data_dir}/train\", transform=transform)\n",
        "val_dataset   = datasets.ImageFolder(root=f\"{data_dir}/val\", transform=transform)\n",
        "test_dataset  = datasets.ImageFolder(root=f\"{data_dir}/test\", transform=transform)\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfmtY8Rj_fYw",
        "outputId": "0436bf1b-6c58-424d-da29-bc611cba7dd0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "# ======================\n",
        "# 2. Model definition\n",
        "# ======================\n",
        "num_classes = 2  # Normal vs Pneumonia\n",
        "model = models.resnet18(pretrained=False)\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyZZQYmC_j0T"
      },
      "outputs": [],
      "source": [
        "# ======================\n",
        "# 3. Loss and optimizer\n",
        "# ======================\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dh2c-22zHCPP"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ======================\n",
        "# 4. Training and evaluation functions\n",
        "# ======================\n",
        "def train_one_epoch(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    return running_loss / len(loader), correct / total\n",
        "\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return running_loss / len(loader), correct / total\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCBkHxSZ_mvi",
        "outputId": "8888f132-476b-4327-9f9d-1245a37223d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss=0.2623, Train Acc=0.8737, Val Loss=2.1448, Val Acc=0.5000\n",
            "Epoch 2: Train Loss=0.0790, Train Acc=0.9785, Val Loss=1.9070, Val Acc=0.5625\n",
            "Epoch 3: Train Loss=0.0410, Train Acc=0.9866, Val Loss=1.6307, Val Acc=0.5625\n",
            "Epoch 4: Train Loss=0.0187, Train Acc=0.9973, Val Loss=1.1242, Val Acc=0.6875\n",
            "Epoch 5: Train Loss=0.0140, Train Acc=0.9987, Val Loss=1.4925, Val Acc=0.6250\n"
          ]
        }
      ],
      "source": [
        "# ======================\n",
        "# 5. Training loop\n",
        "# ======================\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n",
        "    val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
        "    print(f\"Epoch {epoch+1}: \"\n",
        "          f\"Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f}, \"\n",
        "          f\"Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Et_8BfOv_i7B",
        "outputId": "feb823db-8c73-4b4c-bf80-e2eba9d4981b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Test: Loss=1.0738, Acc=0.7228\n"
          ]
        }
      ],
      "source": [
        "# ======================\n",
        "# 6. Final test evaluation with Detailed Metrics\n",
        "# ======================\n",
        "\n",
        "# Get detailed predictions for baseline model\n",
        "def evaluate_with_predictions(model, loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    all_preds, all_labels = [], []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "            \n",
        "            # Store predictions and labels\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    \n",
        "    return running_loss / len(loader), correct / total, all_preds, all_labels\n",
        "\n",
        "# Evaluate baseline model with detailed metrics\n",
        "test_loss, test_acc, test_preds_baseline, test_labels_baseline = evaluate_with_predictions(model, test_loader, criterion)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"BASELINE MODEL RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Final Test: Loss={test_loss:.4f}, Acc={test_acc:.4f}\")\n",
        "print()\n",
        "\n",
        "# Import necessary libraries for detailed metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Class names\n",
        "class_names = ['Normal', 'Pneumonia']\n",
        "\n",
        "# Detailed classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(test_labels_baseline, test_preds_baseline, target_names=class_names))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm_baseline = confusion_matrix(test_labels_baseline, test_preds_baseline)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_baseline, annot=True, fmt='d', cmap='Reds', \n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title('Baseline Model - Confusion Matrix')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()\n",
        "\n",
        "# Calculate per-class metrics\n",
        "precision_baseline, recall_baseline, f1_baseline, support_baseline = precision_recall_fscore_support(test_labels_baseline, test_preds_baseline)\n",
        "\n",
        "print(\"\\nPer-class Metrics:\")\n",
        "for i, class_name in enumerate(class_names):\n",
        "    print(f\"{class_name}:\")\n",
        "    print(f\"  Precision: {precision_baseline[i]:.4f}\")\n",
        "    print(f\"  Recall:    {recall_baseline[i]:.4f}\")\n",
        "    print(f\"  F1-score:  {f1_baseline[i]:.4f}\")\n",
        "    print(f\"  Support:   {support_baseline[i]}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Improved Model Implementation**\n",
        "以下は、Project Deliverableの要件に沿って精度を改善したモデルです。Baselineコードは一切変更しません。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ======================\n",
        "# Improved Model 1: Transfer Learning with Pretrained Weights\n",
        "# ======================\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Improved model with pretrained weights\n",
        "num_classes_improved = 2  # Keep same as baseline for fair comparison\n",
        "model_improved = models.resnet18(pretrained=True)  # Use pretrained weights!\n",
        "model_improved.fc = nn.Linear(model_improved.fc.in_features, num_classes_improved)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_improved = model_improved.to(device)\n",
        "\n",
        "print(\"Improved Model 1: ResNet-18 with ImageNet pretrained weights\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ======================\n",
        "# Improved Data Augmentation for Training\n",
        "# ======================\n",
        "\n",
        "# Enhanced transforms with data augmentation for training\n",
        "train_transform_improved = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomCrop(224),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Keep validation and test transforms same as baseline\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Create improved datasets with augmentation\n",
        "train_dataset_improved = datasets.ImageFolder(root=f\"{data_dir}/train\", transform=train_transform_improved)\n",
        "val_dataset_improved = datasets.ImageFolder(root=f\"{data_dir}/val\", transform=val_test_transform)\n",
        "test_dataset_improved = datasets.ImageFolder(root=f\"{data_dir}/test\", transform=val_test_transform)\n",
        "\n",
        "# Create data loaders with optimized batch size\n",
        "train_loader_improved = DataLoader(train_dataset_improved, batch_size=16, shuffle=True, num_workers=2)\n",
        "val_loader_improved = DataLoader(val_dataset_improved, batch_size=16, shuffle=False, num_workers=2)\n",
        "test_loader_improved = DataLoader(test_dataset_improved, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "print(\"Improved data loaders created with augmentation and optimized batch size\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ======================\n",
        "# Improved Loss and Optimizer with Learning Rate Scheduling\n",
        "# ======================\n",
        "\n",
        "# Calculate class weights for imbalanced dataset\n",
        "def calculate_class_weights(dataset):\n",
        "    class_counts = {}\n",
        "    for _, label in dataset:\n",
        "        class_counts[label] = class_counts.get(label, 0) + 1\n",
        "    \n",
        "    total_samples = len(dataset)\n",
        "    class_weights = []\n",
        "    for i in range(len(class_counts)):\n",
        "        weight = total_samples / (len(class_counts) * class_counts[i])\n",
        "        class_weights.append(weight)\n",
        "    \n",
        "    return torch.FloatTensor(class_weights)\n",
        "\n",
        "# Calculate class weights\n",
        "class_weights = calculate_class_weights(train_dataset_improved)\n",
        "print(f\"Class weights: {class_weights}\")\n",
        "\n",
        "# Improved loss with class weighting\n",
        "criterion_improved = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
        "\n",
        "# Improved optimizer with different learning rate\n",
        "optimizer_improved = optim.Adam(model_improved.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "\n",
        "# Learning rate scheduler (removed verbose parameter as it's not supported in some PyTorch versions)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer_improved, mode='min', factor=0.5, patience=3)\n",
        "\n",
        "print(\"Improved loss, optimizer, and scheduler initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ======================\n",
        "# Improved Training Functions with Detailed Metrics\n",
        "# ======================\n",
        "\n",
        "def train_one_epoch_improved(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    all_preds, all_labels = [], []\n",
        "    \n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "        \n",
        "        # Store predictions and labels for detailed metrics\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "    \n",
        "    return running_loss / len(loader), correct / total, all_preds, all_labels\n",
        "\n",
        "def evaluate_improved(model, loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    all_preds, all_labels = [], []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "            \n",
        "            # Store predictions and labels for detailed metrics\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    \n",
        "    return running_loss / len(loader), correct / total, all_preds, all_labels\n",
        "\n",
        "print(\"Improved training and evaluation functions defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ======================\n",
        "# Improved Training Loop with Early Stopping\n",
        "# ======================\n",
        "\n",
        "# Training history tracking\n",
        "train_losses, train_accs = [], []\n",
        "val_losses, val_accs = [], []\n",
        "best_val_acc = 0.0\n",
        "patience_counter = 0\n",
        "patience = 5\n",
        "\n",
        "num_epochs_improved = 15\n",
        "print(\"Starting improved model training...\")\n",
        "\n",
        "for epoch in range(num_epochs_improved):\n",
        "    # Training\n",
        "    train_loss, train_acc, train_preds, train_labels = train_one_epoch_improved(\n",
        "        model_improved, train_loader_improved, optimizer_improved, criterion_improved)\n",
        "    \n",
        "    # Validation\n",
        "    val_loss, val_acc, val_preds, val_labels = evaluate_improved(\n",
        "        model_improved, val_loader_improved, criterion_improved)\n",
        "    \n",
        "    # Learning rate scheduling\n",
        "    scheduler.step(val_loss)\n",
        "    \n",
        "    # Track history\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accs.append(val_acc)\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}/{num_epochs_improved}: \"\n",
        "          f\"Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f}, \"\n",
        "          f\"Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}\")\n",
        "    \n",
        "    # Early stopping\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        patience_counter = 0\n",
        "        # Save best model\n",
        "        torch.save(model_improved.state_dict(), 'best_model_improved.pth')\n",
        "        print(f\"New best validation accuracy: {best_val_acc:.4f}\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
        "            break\n",
        "\n",
        "print(f\"Training completed. Best validation accuracy: {best_val_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ======================\n",
        "# Load Best Model and Final Evaluation with Detailed Metrics\n",
        "# ======================\n",
        "\n",
        "# Load the best model\n",
        "model_improved.load_state_dict(torch.load('best_model_improved.pth'))\n",
        "\n",
        "# Final test evaluation with detailed metrics\n",
        "test_loss_improved, test_acc_improved, test_preds, test_labels = evaluate_improved(\n",
        "    model_improved, test_loader_improved, criterion_improved)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"IMPROVED MODEL RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Final Test: Loss={test_loss_improved:.4f}, Acc={test_acc_improved:.4f}\")\n",
        "print()\n",
        "\n",
        "# Detailed classification report\n",
        "class_names = ['Normal', 'Pneumonia']  # Adjust based on your dataset classes\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(test_labels, test_preds, target_names=class_names))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(test_labels, test_preds)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title('Improved Model - Confusion Matrix')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()\n",
        "\n",
        "# Calculate per-class metrics\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "precision, recall, f1, support = precision_recall_fscore_support(test_labels, test_preds)\n",
        "\n",
        "print(\"\\nPer-class Metrics:\")\n",
        "for i, class_name in enumerate(class_names):\n",
        "    print(f\"{class_name}:\")\n",
        "    print(f\"  Precision: {precision[i]:.4f}\")\n",
        "    print(f\"  Recall:    {recall[i]:.4f}\")\n",
        "    print(f\"  F1-score:  {f1[i]:.4f}\")\n",
        "    print(f\"  Support:   {support[i]}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ======================\n",
        "# Training History Visualization\n",
        "# ======================\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Plot 1: Loss\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(train_losses, label='Training Loss', color='blue')\n",
        "plt.plot(val_losses, label='Validation Loss', color='red')\n",
        "plt.title('Model Loss Over Time')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot 2: Accuracy\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(train_accs, label='Training Accuracy', color='blue')\n",
        "plt.plot(val_accs, label='Validation Accuracy', color='red')\n",
        "plt.title('Model Accuracy Over Time')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot 3: Learning Rate (if we can access it)\n",
        "plt.subplot(1, 3, 3)\n",
        "current_lr = optimizer_improved.param_groups[0]['lr']\n",
        "plt.axhline(y=current_lr, color='green', linestyle='--', label=f'Final LR: {current_lr:.6f}')\n",
        "plt.title('Learning Rate')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Training visualizations completed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Comparison: Baseline vs Improved Model**\n",
        "\n",
        "以下のセルで、ベースラインモデルと改善されたモデルの性能を比較します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ======================\n",
        "# Detailed Model Comparison with Actual Results\n",
        "# ======================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"DETAILED MODEL PERFORMANCE COMPARISON\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"BASELINE MODEL (ResNet-18, No Pretrained Weights):\")\n",
        "print(\"- Architecture: ResNet-18\")\n",
        "print(\"- Pretrained: No\")\n",
        "print(\"- Data Augmentation: None\")\n",
        "print(\"- Learning Rate: 1e-4\")\n",
        "print(\"- Batch Size: 32\")\n",
        "print(\"- Epochs: 5\")\n",
        "print(\"- Class Weighting: No\")\n",
        "print(\"- Early Stopping: No\")\n",
        "print(f\"- Test Accuracy: {test_acc:.4f}\")\n",
        "print(f\"- Test Loss: {test_loss:.4f}\")\n",
        "print()\n",
        "\n",
        "print(\"IMPROVED MODEL (ResNet-18, Pretrained + Enhancements):\")\n",
        "print(\"- Architecture: ResNet-18\")\n",
        "print(\"- Pretrained: Yes (ImageNet)\")\n",
        "print(\"- Data Augmentation: Yes (Crop, Flip, Rotation, ColorJitter)\")\n",
        "print(\"- Learning Rate: 1e-3 with ReduceLROnPlateau\")\n",
        "print(\"- Batch Size: 16\")\n",
        "print(\"- Epochs: Up to 15 (with early stopping)\")\n",
        "print(\"- Class Weighting: Yes\")\n",
        "print(\"- Early Stopping: Yes (patience=5)\")\n",
        "print(f\"- Test Accuracy: {test_acc_improved:.4f}\")\n",
        "print(f\"- Test Loss: {test_loss_improved:.4f}\")\n",
        "print()\n",
        "\n",
        "# Calculate improvement\n",
        "acc_improvement = test_acc_improved - test_acc\n",
        "acc_improvement_percent = (acc_improvement / test_acc) * 100\n",
        "\n",
        "print(\"PERFORMANCE IMPROVEMENT:\")\n",
        "print(f\"- Accuracy Improvement: {acc_improvement:+.4f} ({acc_improvement_percent:+.2f}%)\")\n",
        "print(f\"- Loss Improvement: {test_loss - test_loss_improved:+.4f}\")\n",
        "print()\n",
        "\n",
        "print(\"DETAILED METRICS COMPARISON:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"{'Metric':<15} {'Baseline':<12} {'Improved':<12} {'Improvement':<12}\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"{'Accuracy':<15} {test_acc:<12.4f} {test_acc_improved:<12.4f} {acc_improvement:+.4f}\")\n",
        "print(f\"{'Loss':<15} {test_loss:<12.4f} {test_loss_improved:<12.4f} {test_loss_improved - test_loss:+.4f}\")\n",
        "\n",
        "# Per-class comparison\n",
        "print(\"\\nPER-CLASS METRICS COMPARISON:\")\n",
        "print(\"-\" * 80)\n",
        "for i, class_name in enumerate(class_names):\n",
        "    print(f\"\\n{class_name.upper()}:\")\n",
        "    print(f\"{'Metric':<12} {'Baseline':<12} {'Improved':<12} {'Improvement':<12}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Calculate improvements for this class\n",
        "    prec_imp = precision[i] - precision_baseline[i]\n",
        "    rec_imp = recall[i] - recall_baseline[i]\n",
        "    f1_imp = f1[i] - f1_baseline[i]\n",
        "    \n",
        "    print(f\"{'Precision':<12} {precision_baseline[i]:<12.4f} {precision[i]:<12.4f} {prec_imp:+.4f}\")\n",
        "    print(f\"{'Recall':<12} {recall_baseline[i]:<12.4f} {recall[i]:<12.4f} {rec_imp:+.4f}\")\n",
        "    print(f\"{'F1-score':<12} {f1_baseline[i]:<12.4f} {f1[i]:<12.4f} {f1_imp:+.4f}\")\n",
        "    print(f\"{'Support':<12} {support_baseline[i]:<12} {support[i]:<12} {support[i] - support_baseline[i]:+}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"KEY IMPROVEMENTS IMPLEMENTED:\")\n",
        "improvements = [\n",
        "    \"✅ Transfer Learning (ImageNet pretrained weights)\",\n",
        "    \"✅ Data Augmentation (Random transforms for training)\",\n",
        "    \"✅ Class-weighted Loss (Handle imbalanced dataset)\",\n",
        "    \"✅ Learning Rate Scheduling (ReduceLROnPlateau)\",\n",
        "    \"✅ Early Stopping (Prevent overfitting)\",\n",
        "    \"✅ Optimized Batch Size (Better memory utilization)\",\n",
        "    \"✅ Detailed Evaluation Metrics (Precision, Recall, F1-score)\",\n",
        "    \"✅ Confusion Matrix Visualization\",\n",
        "    \"✅ Training History Tracking\",\n",
        "    \"✅ Model Checkpointing (Save best model)\"\n",
        "]\n",
        "\n",
        "for i, improvement in enumerate(improvements, 1):\n",
        "    print(f\"{i:2d}. {improvement}\")\n",
        "\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ======================\n",
        "# Visual Comparison of Models\n",
        "# ======================\n",
        "\n",
        "# Create side-by-side comparison plots\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "fig.suptitle('Baseline vs Improved Model Comparison', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 1. Confusion Matrices Comparison\n",
        "axes[0, 0].set_title('Baseline - Confusion Matrix', fontsize=12, fontweight='bold')\n",
        "sns.heatmap(cm_baseline, annot=True, fmt='d', cmap='Reds', \n",
        "            xticklabels=class_names, yticklabels=class_names, ax=axes[0, 0])\n",
        "axes[0, 0].set_ylabel('True Label')\n",
        "axes[0, 0].set_xlabel('Predicted Label')\n",
        "\n",
        "axes[0, 1].set_title('Improved - Confusion Matrix', fontsize=12, fontweight='bold')\n",
        "cm_improved = confusion_matrix(test_labels, test_preds)\n",
        "sns.heatmap(cm_improved, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=class_names, yticklabels=class_names, ax=axes[0, 1])\n",
        "axes[0, 1].set_ylabel('True Label')\n",
        "axes[0, 1].set_xlabel('Predicted Label')\n",
        "\n",
        "# 2. Accuracy Comparison Bar Chart\n",
        "models = ['Baseline', 'Improved']\n",
        "accuracies = [test_acc, test_acc_improved]\n",
        "colors = ['red', 'blue']\n",
        "\n",
        "bars = axes[0, 2].bar(models, accuracies, color=colors, alpha=0.7)\n",
        "axes[0, 2].set_title('Overall Accuracy Comparison', fontsize=12, fontweight='bold')\n",
        "axes[0, 2].set_ylabel('Accuracy')\n",
        "axes[0, 2].set_ylim(0, 1)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, acc in zip(bars, accuracies):\n",
        "    height = bar.get_height()\n",
        "    axes[0, 2].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                    f'{acc:.4f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# 3. Per-class Precision Comparison\n",
        "x = np.arange(len(class_names))\n",
        "width = 0.35\n",
        "\n",
        "prec_baseline_plot = axes[1, 0].bar(x - width/2, precision_baseline, width, \n",
        "                                    label='Baseline', color='red', alpha=0.7)\n",
        "prec_improved_plot = axes[1, 0].bar(x + width/2, precision, width, \n",
        "                                    label='Improved', color='blue', alpha=0.7)\n",
        "\n",
        "axes[1, 0].set_title('Precision Comparison by Class', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_ylabel('Precision')\n",
        "axes[1, 0].set_xlabel('Class')\n",
        "axes[1, 0].set_xticks(x)\n",
        "axes[1, 0].set_xticklabels(class_names)\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].set_ylim(0, 1)\n",
        "\n",
        "# 4. Per-class Recall Comparison\n",
        "rec_baseline_plot = axes[1, 1].bar(x - width/2, recall_baseline, width, \n",
        "                                   label='Baseline', color='red', alpha=0.7)\n",
        "rec_improved_plot = axes[1, 1].bar(x + width/2, recall, width, \n",
        "                                   label='Improved', color='blue', alpha=0.7)\n",
        "\n",
        "axes[1, 1].set_title('Recall Comparison by Class', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].set_ylabel('Recall')\n",
        "axes[1, 1].set_xlabel('Class')\n",
        "axes[1, 1].set_xticks(x)\n",
        "axes[1, 1].set_xticklabels(class_names)\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].set_ylim(0, 1)\n",
        "\n",
        "# 5. Per-class F1-score Comparison\n",
        "f1_baseline_plot = axes[1, 2].bar(x - width/2, f1_baseline, width, \n",
        "                                  label='Baseline', color='red', alpha=0.7)\n",
        "f1_improved_plot = axes[1, 2].bar(x + width/2, f1, width, \n",
        "                                  label='Improved', color='blue', alpha=0.7)\n",
        "\n",
        "axes[1, 2].set_title('F1-Score Comparison by Class', fontsize=12, fontweight='bold')\n",
        "axes[1, 2].set_ylabel('F1-Score')\n",
        "axes[1, 2].set_xlabel('Class')\n",
        "axes[1, 2].set_xticks(x)\n",
        "axes[1, 2].set_xticklabels(class_names)\n",
        "axes[1, 2].legend()\n",
        "axes[1, 2].set_ylim(0, 1)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Visual comparison completed!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
